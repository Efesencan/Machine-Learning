{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTfDIhAYYjaX"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HGi6c-0uZSeB",
    "outputId": "ea63783e-fb4f-43e8-f4a5-d89eb75b43c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFpeyxi8aqIl"
   },
   "outputs": [],
   "source": [
    "# You can find the data under https://drive.google.com/drive/folders/1e550az93U3_kfRBbVY5PZnMKYwGYmHqi?usp=sharing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"/content/drive/My Drive/HW1/train_data.csv\")  # One line of code\n",
    "train_label = pd.read_csv(\"/content/drive/My Drive/HW1/train_label.csv\")  # One line of code\n",
    "\n",
    "test_data = pd.read_csv(\"/content/drive/My Drive/HW1/test_data.csv\")  # One line of code\n",
    "test_label = pd.read_csv(\"/content/drive/My Drive/HW1/test_label.csv\")  # One line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "colab_type": "code",
    "id": "OZXqzZlXBIN0",
    "outputId": "de28ae14-abdb-40b6-e03e-0f5b5a3cbbca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (793, 49)\n",
      "Test Data:  (207, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>f_worker</th>\n",
       "      <th>checking_status_&lt;0</th>\n",
       "      <th>checking_status_&gt;=200</th>\n",
       "      <th>checking_status_no checking</th>\n",
       "      <th>credit_history_critical/other existing credit</th>\n",
       "      <th>credit_history_delayed previously</th>\n",
       "      <th>credit_history_existing paid</th>\n",
       "      <th>credit_history_no credits/all paid</th>\n",
       "      <th>purpose_domestic appliance</th>\n",
       "      <th>purpose_education</th>\n",
       "      <th>purpose_furniture/equipment</th>\n",
       "      <th>purpose_new car</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_radio/tv</th>\n",
       "      <th>purpose_repairs</th>\n",
       "      <th>purpose_retraining</th>\n",
       "      <th>purpose_used car</th>\n",
       "      <th>savings_status_500&lt;=X&lt;1000</th>\n",
       "      <th>savings_status_&lt;100</th>\n",
       "      <th>savings_status_&gt;=1000</th>\n",
       "      <th>savings_status_no known savings</th>\n",
       "      <th>employment_4&lt;=X&lt;7</th>\n",
       "      <th>employment_&lt;1</th>\n",
       "      <th>employment_&gt;=7</th>\n",
       "      <th>employment_unemployed</th>\n",
       "      <th>personal_status_male div/sep</th>\n",
       "      <th>personal_status_male mar/wid</th>\n",
       "      <th>personal_status_male single</th>\n",
       "      <th>other_parties_guarantor</th>\n",
       "      <th>other_parties_none</th>\n",
       "      <th>property_magnitude_life insurance</th>\n",
       "      <th>property_magnitude_no known property</th>\n",
       "      <th>property_magnitude_real estate</th>\n",
       "      <th>other_payment_plans_none</th>\n",
       "      <th>other_payment_plans_stores</th>\n",
       "      <th>housing_own</th>\n",
       "      <th>housing_rent</th>\n",
       "      <th>job_skilled</th>\n",
       "      <th>job_unemp/unskilled non res</th>\n",
       "      <th>job_unskilled resident</th>\n",
       "      <th>own_telephone_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>561</td>\n",
       "      <td>24</td>\n",
       "      <td>2964</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>226</td>\n",
       "      <td>36</td>\n",
       "      <td>2613</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>24</td>\n",
       "      <td>1603</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>493</td>\n",
       "      <td>6</td>\n",
       "      <td>1237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>598</td>\n",
       "      <td>24</td>\n",
       "      <td>4241</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>324</td>\n",
       "      <td>18</td>\n",
       "      <td>2659</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>704</td>\n",
       "      <td>30</td>\n",
       "      <td>2503</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>424</td>\n",
       "      <td>21</td>\n",
       "      <td>1591</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>708</td>\n",
       "      <td>12</td>\n",
       "      <td>2969</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>12</td>\n",
       "      <td>618</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  duration  ...  job_unskilled resident  own_telephone_yes\n",
       "560  561        24  ...                       0                  1\n",
       "225  226        36  ...                       0                  0\n",
       "261  262        24  ...                       0                  0\n",
       "492  493         6  ...                       0                  0\n",
       "597  598        24  ...                       1                  1\n",
       "323  324        18  ...                       0                  0\n",
       "703  704        30  ...                       0                  0\n",
       "423  424        21  ...                       0                  0\n",
       "707  708        12  ...                       0                  0\n",
       "90    91        12  ...                       0                  0\n",
       "\n",
       "[10 rows x 49 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show random samples from the training data\n",
    "#print(train_data.shape)\n",
    "# One line of code\n",
    "print(\"Training Data: \",train_data.shape)\n",
    "train_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQIVU4-zXpE_"
   },
   "source": [
    "# Train Decision Tree with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oLznLl_lXYZf",
    "outputId": "e57b542d-6698-43ea-816c-e7931ed94474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 67.14976%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train decision tree using the whole training data with **entropy** criteria\n",
    "\n",
    "# One line of code\n",
    "clf = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "# One line of code\n",
    "clf = clf.fit(train_data, train_label)\n",
    "\n",
    "# Estimate the prediction of test data\n",
    "test_pred = clf.predict(test_data)   # One line of code\n",
    "\n",
    "# Calculate accuracy of test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "TestAcc = accuracy_score(test_label, test_pred)# One line of code\n",
    "print(\"Testing Accuracy = %.5f%%\" % (TestAcc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqgNZYUMXv8X"
   },
   "source": [
    "# FineTune Decision Tree parameters\n",
    "\n",
    "1- Spliting dataset into train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWJxk-zjy0Kv"
   },
   "outputs": [],
   "source": [
    "# Split training data to 70% training and 30% validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.3)# One line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqws-kTZYHoG"
   },
   "source": [
    "2- FineTune minimum sample split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "O1DvpmCCJXTb",
    "outputId": "24f2644a-f859-4c33-87c7-07eb63af79e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZeriZ1iEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_samples_splits = range(2, 100)\n",
    "\n",
    "train_results = []\n",
    "val_results = []\n",
    "for min_samples_split in min_samples_splits:\n",
    "  \n",
    "  # Fit the tree using the 70% portion of the training data\n",
    "  # One line of code\n",
    "  # One line of code\n",
    "  clf_tune = DecisionTreeClassifier(criterion='entropy',min_samples_split = min_samples_split,random_state=0)\n",
    "  clf_tune = clf_tune.fit(x_train, y_train)\n",
    "  \n",
    "  # Evaluate on Training set\n",
    "  train_pred = clf_tune.predict(x_train) # One line of code\n",
    "  train_acc =  accuracy_score(y_train, train_pred) # One line of code\n",
    "  train_results.append(train_acc)\n",
    "   \n",
    "  # Evaluate on Validation set\n",
    "  val_pred = clf_tune.predict(x_val) # One line of code\n",
    "  val_acc =  accuracy_score(y_val, val_pred)  # One line of code\n",
    "  val_results.append(val_acc)\n",
    "  \n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(min_samples_splits, train_results, 'b')\n",
    "plt.plot(min_samples_splits, val_results,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "akVAE3MbL7bE",
    "outputId": "0b8bd942-a27e-4e93-cccd-7df4aac2697e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best min sample split: 40\n",
      "Testing Accuracy = 71.98068%\n"
     ]
    }
   ],
   "source": [
    "# Choose the best minimum split sample based on the plot\n",
    "Best_minSampl = np.argmax(val_results) # One line of code\n",
    "print(\"Best min sample split:\",min_samples_splits[Best_minSampl])\n",
    "# Train decision tree using the full training data and the best minimum split sample\n",
    "# One line of code\n",
    "clf_best = DecisionTreeClassifier(criterion='entropy',min_samples_split=min_samples_splits[Best_minSampl],random_state = 0)\n",
    "# One line of code\n",
    "clf_best = clf_best.fit(train_data, train_label)\n",
    "\n",
    "# Estimate the prediction of the test data\n",
    "test_pred = clf_best.predict(test_data)# One line of code\n",
    "\n",
    "# Calculate accuracy of test data\n",
    "TestAcc = accuracy_score(test_label, test_pred) # One line of code\n",
    "print(\"Testing Accuracy = %.5f%%\" % (TestAcc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "YZIqdzm9q-gL",
    "outputId": "cc02828b-6af7-4fde-becb-5bbd869753a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Bad Credit</th>\n",
       "      <th>Predicted Good Credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Bad Credit</th>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Good Credit</th>\n",
       "      <td>21</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Bad Credit  Predicted Good Credit\n",
       "True Bad Credit                     25                     37\n",
       "True Good Credit                    21                    124"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(test_label, test_pred),\n",
    "    columns=['Predicted Bad Credit', 'Predicted Good Credit'],\n",
    "    index=['True Bad Credit', 'True Good Credit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VciE0lfYKhya"
   },
   "source": [
    "# Now, apply the same procedure but using KNN instead of decision tree \n",
    "\n",
    "# For finetuning, find the best value of K to use with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "EdBaXNKoLBcL",
    "outputId": "f7314519-52b0-49d6-af2b-8910b335e36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k-value: 29\n",
      "Testing Accuracy = 70.53140%\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize the values of k to be a list of odd numbers between 1 and 30\n",
    "kVals = range(1,30,2)\n",
    "\n",
    "# Save the accuracies of each value of kVal in [accuracies] variable\n",
    "# hint: you can use accuracies.append(...) function inside the loop\n",
    "accuracies = []\n",
    "\n",
    "# loop over values of k for the k-Nearest Neighbor classifier\n",
    "for k in kVals:\n",
    "\n",
    "  # Follow what we did in decision tree part\n",
    "  clf_n = KNeighborsClassifier(n_neighbors=k)  # trained the knn using the 70% portion of the training data\n",
    "  clf_n.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "  # Evaluate on Validation set\n",
    "  result = clf_n.score(x_val,y_val) # One line of code \n",
    "  accuracies.append(result)         # One line of code\n",
    "\n",
    "\n",
    "# Train KNN using the full training data with the best K that you found\n",
    "i = np.argmax(accuracies)\n",
    "print(\"Optimal k-value:\",kVals[i])\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(train_data, train_label.values.ravel())\n",
    "\n",
    "# Predict the labels of the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Testing\n",
    "TestAccuracy = accuracy_score(test_label, predictions)\n",
    "print(\"Testing Accuracy = %.5f%%\" % (TestAccuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "UIXzfzUJsW7x",
    "outputId": "10ac525f-19c2-4e02-de12-0539542abcf5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Bad Credit</th>\n",
       "      <th>Predicted Good Credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Bad Credit</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Good Credit</th>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Bad Credit  Predicted Good Credit\n",
       "True Bad Credit                      5                     57\n",
       "True Good Credit                     4                    141"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(test_label, predictions),\n",
    "    columns=['Predicted Bad Credit', 'Predicted Good Credit'],\n",
    "    index=['True Bad Credit', 'True Good Credit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMEdOIlJQBin"
   },
   "source": [
    "# Bonus\n",
    "\n",
    "# Apply gridsearch using decision tree on any hyperparameter(s) of your choice, you have to beat your previous obtained accuracies to get the bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "J-9TxVbXQCw7",
    "outputId": "2ab262dc-7b76-406a-f21e-e4dd97bbf95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decsT__criterion': 'entropy', 'decsT__max_depth': 8, 'decsT__min_samples_split': 22}\n",
      "Testing Accuracy = 74.39614%\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_steps = [(\"scaler\",StandardScaler()),(\"decsT\",DecisionTreeClassifier(random_state=0))]\n",
    "\n",
    "check_params = {\"decsT__criterion\": [\"gini\",\"entropy\"],\n",
    "                \"decsT__max_depth\": np.arange(3,25),\n",
    "                \"decsT__min_samples_split\": np.arange(2,40)\n",
    "                }\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "#print(pipeline)\n",
    "\n",
    "create_grid = GridSearchCV(pipeline, param_grid = check_params)\n",
    "create_grid.fit(train_data, train_label) # was x_train, y_train\n",
    "print(create_grid.best_params_)\n",
    "\n",
    "clf_grid = DecisionTreeClassifier(criterion=create_grid.best_params_[\"decsT__criterion\"] , min_samples_split = create_grid.best_params_[\"decsT__min_samples_split\"], max_depth=create_grid.best_params_[\"decsT__max_depth\"],random_state=0)\n",
    "# One line of code\n",
    "clf_grid = clf_grid.fit(train_data, train_label)\n",
    "\n",
    "# Estimate the prediction of the test data\n",
    "test_predd = clf_grid.predict(test_data)# One line of code\n",
    "\n",
    "# Calculate accuracy of test data\n",
    "TestAcc = accuracy_score(test_label, test_predd)\n",
    "print(\"Testing Accuracy = %.5f%%\" % (TestAcc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VCH10cwnAU-"
   },
   "source": [
    "# Report: Write a summary of your approach to this problem; this should be like an abstract of a paper or the executive summary (you aim for clarity and passing on information, not going to details about known facts such as what decision trees are, assuming they are known to people in your research area).\n",
    "\n",
    "Must include statements such as:\n",
    "\n",
    "\n",
    "*   Include the problem definition: 1-2 lines\n",
    "*   Talk about train/val/test sets, size and how split.\n",
    "*   State what your test results are with the chosen method, parameters: e.g. \"We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data….\"\n",
    "*   Comment on the speed of the algorithms and anything else that you deem important/interesting (e.g. confusion matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72KDasqHnt1T"
   },
   "source": [
    "# Report\n",
    "\n",
    "**Problem Definition:**\n",
    "  We were given a dataset which consist of the personal information and credit card history of a person, and our goal here was to make a binary classification on whether a single person has good or bad credit risk based on that data. \n",
    "\n",
    "  **Methods Used**:\n",
    "\n",
    "  Decision Tree:\n",
    "\n",
    "  We first used the decision tree for the classification without spesifiying any (used default parameters with {criterion: entropy ,random state = 0})hyperparameters and used the whole data sets for training. Our training data consist of 793 rows and 49 columns. After we perform the training, we tested our model where our test data consist of 207 rows and 49 columns, and the accuracy rate was 67.14976%. After that, we performed fine tuning based on minimum sample splits ranging between (2,100). For fine,tuning;\n",
    "we splitted 70% of our training data for training and 30% for the validation data. After performing the training and testing part using the best minimum sample split result which was 40, we get 71.98068% as an accuracy rate.\n",
    "\n",
    "  KNN:\n",
    "\n",
    "  The second method we used was KNN. We tried to find the k-value which will give us the highest accuracy rate. Our k-values were the odd numbers ranged between (1,30). After performing fine tuning we found our optimal k value as 29 and trained KNN using the full training data. We get 70.53140% as an accuracy rate.\n",
    "\n",
    "  **Grid Search:**\n",
    "\n",
    "  In order to improve our accuracy score in our decision tree model, we performed grid search and try to optimize the hyperparameters. We have obtained the best accuracy score as 74.39614% after setting the parameters such that (decision criteria = entropy, max_depth = 8, and min_samples = 22). The time complexity of this grid search and prediction was longer compared with the other models since it looks for multiple hyperparameter values and tries to find the property that maximizes the overall accuracy score.\n",
    "\n",
    "**Confusion Matrix:**\n",
    "\n",
    "If we observe the confusion matrix results which were generated in the above cells, for the decision tree classifer; we can see that our model predicted 37 person as good credit but they should be considered to have bad credit. For the KNN 55 people was considered to have a good credit but they should have bad credit in the actual case. For this reason, decision tree performed better compared with the KNN model, since giving good credit score to a bad score is more risky than giving bad score to a good credit score. Overall, both of our models performed well for predicting the good credit score of a person. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "..\n",
    "\n",
    ".."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Efesencan-HW1-GermanCreditCard.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
